{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    '''\n",
    "    Transformer Block combines both the attention module and the feed forward module with layer\n",
    "    normalization, dropout and residual connections. The sequence of operations is as follows :-\n",
    "\n",
    "    Input -> LayerNorm1 -> Attention -> Residual -> LayerNorm2 -> FeedForward -> Output\n",
    "      |                                   |  |                                      |\n",
    "      |-------------Addition--------------|  |---------------Addition---------------|\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 heads=3,           # modify 8 to 3\n",
    "                 mlp_ratio=4,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 activation=nn.GELU,\n",
    "                 norm_layer = nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embed_dim)\n",
    "        self.attn = Attention(embed_dim, heads=heads,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        self.drop_path = DropPath(drop_path_ratio)\n",
    "        self.norm2 = norm_layer(embed_dim)\n",
    "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=embed_dim, hidden_features=mlp_hidden_dim, act_layer=activation, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def random_indexes(size : int):\n",
    "    forward_indexes = np.arange(size)\n",
    "    np.random.shuffle(forward_indexes)\n",
    "    backward_indexes = np.argsort(forward_indexes)\n",
    "    return forward_indexes, backward_indexes\n",
    "\n",
    "def take_indexes(sequences, indexes):\n",
    "    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n",
    "\n",
    "class PatchShuffle(torch.nn.Module):\n",
    "    def __init__(self, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches : torch.Tensor):\n",
    "        T, B, C = patches.shape      # length, batch, dim\n",
    "        remain_T = int(T * (1 - self.ratio))\n",
    "\n",
    "        indexes = [random_indexes(T) for _ in range(B)]\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)\n",
    "        patches = patches[:remain_T] #The pacth is not masked [T*0.25, B, C]\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=32,\n",
    "                 patch_size=2,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=12,\n",
    "                 num_head=3,\n",
    "                 mask_ratio=0.75,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n",
    "\n",
    "        #  mask the patch and shuffle it\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        # we get the (3, dim, patch, patch)\n",
    "        self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        # laynorm of ViT\n",
    "        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "\n",
    "        return features, backward_indexes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=32,\n",
    "                 patch_size=2,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=4,\n",
    "                 num_head=3,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.head = torch.nn.Linear(emb_dim, 3 * patch_size ** 2)\n",
    "        self.patch2img = Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size//patch_size)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.mask_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        T = features.shape[0]\n",
    "        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n",
    "        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = rearrange(features, 't b c -> b t c')\n",
    "        features = self.transformer(features)\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        features = features[1:] # remove global feature\n",
    "\n",
    "        patches = self.head(features) # get patchs by head\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[T:] = 1  # mask the other pixels to 1\n",
    "        mask = take_indexes(mask, backward_indexes[1:] - 1)\n",
    "        img = self.patch2img(patches) #\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=32,\n",
    "                 patch_size=2,\n",
    "                 emb_dim=192,\n",
    "                 encoder_layer=12,\n",
    "                 encoder_head=3,\n",
    "                 decoder_layer=4,\n",
    "                 decoder_head=3,\n",
    "                 mask_ratio=0.75,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, encoder_layer, encoder_head, mask_ratio)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, decoder_layer, decoder_head)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        predicted_img, mask = self.decoder(features,  backward_indexes)\n",
    "        return predicted_img, mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "def setup_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed = 2022\n",
    "setup_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "load_batch_size = batch_size\n",
    "train_dataset = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=Compose([ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "val_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, transform=Compose([ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, load_batch_size, shuffle=True, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_random_img(dataset):\n",
    "    total = len(dataset)\n",
    "    random = np.random.randint(total)\n",
    "\n",
    "    img,label = dataset[random]\n",
    "\n",
    "    return img, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mask_ratio = 0.75\n",
    "model = MAE_ViT(mask_ratio = mask_ratio).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(model,(8,3,32,32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writer = SummaryWriter(os.path.join('logs', 'cifar10', 'mae-pretrain'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_learning_rate = 1.5e-4\n",
    "weight_decay = 5e-2\n",
    "warmup_epoch = 2\n",
    "total_epoch = 20\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=base_learning_rate * batch_size / 512, betas=(0.9, 0.95), weight_decay=weight_decay)      #modify 256 to 512\n",
    "lr_func = lambda epoch: min((epoch + 1) / (warmup_epoch + 1e-8), 0.5 * (math.cos(epoch / total_epoch * math.pi) + 1))\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_func, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path =  'model/vit-t-mae.pth'  # Pretrained model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "for e in range(total_epoch):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        train_step = len(dataloader)\n",
    "        with tqdm(total=train_step,desc=f'Epoch {e+1}/{total_epoch}',postfix=dict,mininterval=0.3) as pbar:\n",
    "            for step,(img, label) in enumerate(iter(dataloader)):\n",
    "\n",
    "                img = img.to(device)\n",
    "                predicted_img, mask = model(img)\n",
    "                optim.zero_grad()\n",
    "                loss = torch.mean((predicted_img - img) ** 2 * mask) / mask_ratio\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                pbar.set_postfix(**{'Train Loss' : np.mean(losses)})\n",
    "                pbar.update(1)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        writer.add_scalar('mae_loss', avg_loss, global_step=e)\n",
    "        print(f'In epoch {e}, average traning loss is {avg_loss}.')\n",
    "\n",
    "        ''' visualize the first 16 predicted images on val dataset'''\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_img = torch.stack([val_dataset[i][0] for i in range(16)])\n",
    "            val_img = val_img.to(device)\n",
    "            predicted_val_img, mask = model(val_img)\n",
    "            predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
    "            img = torch.cat([val_img * (1 - mask), predicted_val_img, val_img], dim=0)\n",
    "            img = rearrange(img, '(v h1 w1) c h w -> c (h1 h) (w1 v w)', w1=2, v=3)\n",
    "            writer.add_image('mae_image', (img + 1) / 2, global_step=e)\n",
    "\n",
    "        ''' save model '''\n",
    "        torch.save(model, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Epoch 1/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.153]\n",
    "Adjusting learning rate of group 0 to 1.4633e-04.\n",
    "In epoch 0, average traning loss is 0.1527432632841626.\n",
    "Epoch 2/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.133]\n",
    "Adjusting learning rate of group 0 to 1.4183e-04.\n",
    "In epoch 1, average traning loss is 0.13297311121559874.\n",
    "Epoch 3/20: 100%\n",
    "196/196 [02:15<00:00, 1.46it/s, Train Loss=0.121]\n",
    "Adjusting learning rate of group 0 to 1.3568e-04.\n",
    "In epoch 2, average traning loss is 0.1214919461659631.\n",
    "Epoch 4/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.107]\n",
    "Adjusting learning rate of group 0 to 1.2803e-04.\n",
    "In epoch 3, average traning loss is 0.1074896129038261.\n",
    "Epoch 5/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.0923]\n",
    "Adjusting learning rate of group 0 to 1.1908e-04.\n",
    "In epoch 4, average traning loss is 0.092297316471837.\n",
    "Epoch 6/20: 100%\n",
    "196/196 [02:15<00:00, 1.46it/s, Train Loss=0.0773]\n",
    "Adjusting learning rate of group 0 to 1.0905e-04.\n",
    "In epoch 5, average traning loss is 0.07725086207596624.\n",
    "Epoch 7/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.0709]\n",
    "Adjusting learning rate of group 0 to 9.8176e-05.\n",
    "In epoch 6, average traning loss is 0.07092976463692528.\n",
    "Epoch 8/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.0665]\n",
    "Adjusting learning rate of group 0 to 8.6733e-05.\n",
    "In epoch 7, average traning loss is 0.06653540098697555.\n",
    "Epoch 9/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.0636]\n",
    "Adjusting learning rate of group 0 to 7.5000e-05.\n",
    "In epoch 8, average traning loss is 0.06363394364182438.\n",
    "Epoch 10/20: 100%\n",
    "196/196 [02:15<00:00, 1.45it/s, Train Loss=0.0613]\n",
    "Adjusting learning rate of group 0 to 6.3267e-05.\n",
    "In epoch 9, average traning loss is 0.0612991193917637.\n",
    "Epoch 11/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.0595]\n",
    "Adjusting learning rate of group 0 to 5.1824e-05.\n",
    "In epoch 10, average traning loss is 0.05954847100894062.\n",
    "Epoch 12/20: 100%\n",
    "196/196 [02:15<00:00, 1.47it/s, Train Loss=0.058]\n",
    "Adjusting learning rate of group 0 to 4.0951e-05.\n",
    "In epoch 11, average traning loss is 0.057993074346865923.\n",
    "Epoch 13/20: 100%\n",
    "196/196 [02:15<00:00, 1.45it/s, Train Loss=0.0569]\n",
    "Adjusting learning rate of group 0 to 3.0916e-05.\n",
    "In epoch 12, average traning loss is 0.056920417546465686.\n",
    "Epoch 14/20: 100%\n",
    "196/196 [02:15<00:00, 1.46it/s, Train Loss=0.0559]\n",
    "Adjusting learning rate of group 0 to 2.1967e-05.\n",
    "In epoch 13, average traning loss is 0.055868666619062424.\n",
    "Epoch 15/20: 100%\n",
    "196/196 [02:15<00:00, 1.46it/s, Train Loss=0.0551]\n",
    "Adjusting learning rate of group 0 to 1.4324e-05.\n",
    "In epoch 14, average traning loss is 0.05505580523488473.\n",
    "Epoch 16/20: 100%\n",
    "196/196 [02:15<00:00, 1.46it/s, Train Loss=0.0545]\n",
    "Adjusting learning rate of group 0 to 8.1745e-06.\n",
    "In epoch 15, average traning loss is 0.05449973943890357.\n",
    "Epoch 17/20: 100%\n",
    "196/196 [02:15<00:00, 1.46it/s, Train Loss=0.054]\n",
    "Adjusting learning rate of group 0 to 3.6708e-06.\n",
    "In epoch 16, average traning loss is 0.05396941450557539.\n",
    "Epoch 18/20: 100%\n",
    "196/196 [02:15<00:00, 1.46it/s, Train Loss=0.0536]\n",
    "Adjusting learning rate of group 0 to 9.2337e-07.\n",
    "In epoch 17, average traning loss is 0.053600318881930134.\n",
    "Epoch 19/20: 100%\n",
    "196/196 [02:15<00:00, 1.46it/s, Train Loss=0.0535]\n",
    "Adjusting learning rate of group 0 to 0.0000e+00.\n",
    "In epoch 18, average traning loss is 0.053503734116651575.\n",
    "Epoch 20/20: 100%\n",
    "196/196 [02:15<00:00, 1.45it/s, Train Loss=0.0534]\n",
    "Adjusting learning rate of group 0 to 9.2337e-07.\n",
    "In epoch 19, average traning loss is 0.05337169821545178."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we can get the value of the loss is 0.053 after 20 epoches of training.\n",
    "pretrained_model_path = model_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  Finetune\n",
    "# MAE finetune model and ViT model are the same. The difference lies in the follow-up processing. ViT extracts cls_token for classification. The MAE finetune model is to mean patches tokens (other than cls tokens) and classify them\n",
    "setup_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = load_batch_size = 128\n",
    "train_dataset = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=Compose([ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "val_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, transform=Compose([ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, load_batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, load_batch_size, shuffle=False, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_learning_rate = 1e-3\n",
    "total_epoch = 5\n",
    "weight_decay = 0.05\n",
    "warmup_epoch = 5\n",
    "pretrained_model_path = model_path\n",
    "output_model_path = 'vit-t-classifier.pth'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ViT_Classifier(torch.nn.Module):\n",
    "    def __init__(self, encoder : MAE_Encoder, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embedding = encoder.pos_embedding\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.transformer\n",
    "        self.layer_norm = encoder.layer_norm\n",
    "        self.head = torch.nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        logits = self.head(features[0])\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ViT_Classifier(model.encoder, num_classes=1).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(model,(8, 3, 32, 32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "==========================================================================================\n",
    "Layer (type:depth-idx)                   Output Shape              Param #\n",
    "==========================================================================================\n",
    "ViT_Classifier                           [8, 10]                   49,344\n",
    "├─Conv2d: 1-1                            [8, 192, 16, 16]          2,496\n",
    "├─Sequential: 1-2                        [8, 257, 192]             --\n",
    "│    └─Block: 2-1                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-1               [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-2               [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-3                [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-4                [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-5               [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-6                     [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-7                [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-8                [8, 257, 192]             --\n",
    "│    └─Block: 2-2                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-9               [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-10              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-11               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-12               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-13              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-14                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-15               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-16               [8, 257, 192]             --\n",
    "│    └─Block: 2-3                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-17              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-18              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-19               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-20               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-21              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-22                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-23               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-24               [8, 257, 192]             --\n",
    "│    └─Block: 2-4                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-25              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-26              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-27               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-28               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-29              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-30                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-31               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-32               [8, 257, 192]             --\n",
    "│    └─Block: 2-5                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-33              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-34              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-35               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-36               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-37              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-38                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-39               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-40               [8, 257, 192]             --\n",
    "│    └─Block: 2-6                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-41              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-42              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-43               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-44               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-45              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-46                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-47               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-48               [8, 257, 192]             --\n",
    "│    └─Block: 2-7                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-49              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-50              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-51               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-52               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-53              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-54                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-55               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-56               [8, 257, 192]             --\n",
    "│    └─Block: 2-8                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-57              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-58              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-59               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-60               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-61              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-62                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-63               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-64               [8, 257, 192]             --\n",
    "│    └─Block: 2-9                        [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-65              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-66              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-67               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-68               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-69              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-70                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-71               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-72               [8, 257, 192]             --\n",
    "│    └─Block: 2-10                       [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-73              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-74              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-75               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-76               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-77              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-78                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-79               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-80               [8, 257, 192]             --\n",
    "│    └─Block: 2-11                       [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-81              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-82              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-83               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-84               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-85              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-86                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-87               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-88               [8, 257, 192]             --\n",
    "│    └─Block: 2-12                       [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-89              [8, 257, 192]             384\n",
    "│    │    └─Attention: 3-90              [8, 257, 192]             147,648\n",
    "│    │    └─Identity: 3-91               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-92               [8, 257, 192]             --\n",
    "│    │    └─LayerNorm: 3-93              [8, 257, 192]             384\n",
    "│    │    └─Mlp: 3-94                    [8, 257, 192]             295,872\n",
    "│    │    └─Identity: 3-95               [8, 257, 192]             --\n",
    "│    │    └─Identity: 3-96               [8, 257, 192]             --\n",
    "├─LayerNorm: 1-3                         [8, 257, 192]             384\n",
    "├─Linear: 1-4                            [8, 10]                   1,930\n",
    "==========================================================================================\n",
    "Total params: 5,385,610\n",
    "Trainable params: 5,385,610\n",
    "Non-trainable params: 0\n",
    "Total mult-adds (M): 47.78\n",
    "==========================================================================================\n",
    "Input size (MB): 0.10\n",
    "Forward/backward pass size (MB): 423.16\n",
    "Params size (MB): 21.35\n",
    "Estimated Total Size (MB): 444.61\n",
    "==========================================================================================\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if pretrained_model_path is not None:\n",
    "    model = torch.load(pretrained_model_path, map_location='cpu')\n",
    "    writer = SummaryWriter(os.path.join('logs', 'cifar10', 'pretrain-cls'))\n",
    "else:\n",
    "    model = MAE_ViT()\n",
    "    writer = SummaryWriter(os.path.join('logs', 'cifar10', 'scratch-cls'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "acc_fn = lambda logit, label: torch.mean((logit.argmax(dim=-1) == label).float())\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=base_learning_rate * batch_size / 512, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "lr_func = lambda epoch: min((epoch + 1) / (warmup_epoch + 1e-8), 0.5 * (math.cos(epoch / total_epoch * math.pi) + 1))\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_func, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "step_count = 0\n",
    "optim.zero_grad()\n",
    "for e in range(total_epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    acces = []\n",
    "    train_step = len(train_dataloader)\n",
    "    with tqdm(total=train_step,desc=f'Train Epoch {e+1}/{total_epoch}',postfix=dict,mininterval=0.3) as pbar:\n",
    "        for step,(img, label) in enumerate(iter(train_dataloader)):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            logits = model(img)\n",
    "            loss = loss_fn(logits, label)\n",
    "            acc = acc_fn(logits, label)\n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "            acces.append(acc.item())\n",
    "\n",
    "            pbar.set_postfix(**{'Train Loss' : np.mean(losses),\n",
    "                      'Tran accs': np.mean(acces)})\n",
    "            pbar.update(1)\n",
    "    lr_scheduler.step()\n",
    "    avg_train_loss = sum(losses) / len(losses)\n",
    "    avg_train_acc = sum(acces) / len(acces)\n",
    "    print(f'In epoch {e}, average training loss is {avg_train_loss}, average training acc is {avg_train_acc}.')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        acces = []\n",
    "        val_step = len(val_dataloader)\n",
    "        with tqdm(total=val_step,desc=f'Val Epoch {e+1}/{total_epoch}',postfix=dict,mininterval=0.3) as pbar2:\n",
    "            for img, label in tqdm(iter(val_dataloader)):\n",
    "                img = img.to(device)\n",
    "                label = label.to(device)\n",
    "                logits = model(img)\n",
    "                loss = loss_fn(logits, label)\n",
    "                acc = acc_fn(logits, label)\n",
    "                losses.append(loss.item())\n",
    "                acces.append(acc.item())\n",
    "                pbar2.set_postfix(**{'Val Loss' : np.mean(losses),\n",
    "                          'Val accs': np.mean(acces)})\n",
    "                pbar2.update(1)\n",
    "        avg_val_loss = sum(losses) / len(losses)\n",
    "        avg_val_acc = sum(acces) / len(acces)\n",
    "        print(f'In epoch {e}, average validation loss is {avg_val_loss}, average validation acc is {avg_val_acc}.')\n",
    "\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        best_val_acc = avg_val_acc\n",
    "        print(f'saving best model with acc {best_val_acc} at {e} epoch!')\n",
    "        torch.save(model, output_model_path)\n",
    "\n",
    "    writer.add_scalars('cls/loss', {'train' : avg_train_loss, 'val' : avg_val_loss}, global_step=e)\n",
    "    writer.add_scalars('cls/acc', {'train' : avg_train_acc, 'val' : avg_val_acc}, global_step=e)\n",
    "\n",
    "    #Finally, we save the best model with acc 0.6529865506329114 at 4 epoch!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Train Epoch 1/5: 100%\n",
    "391/391 [03:54<00:00, 1.90it/s, Train Loss=1.55, Tran accs=0.422]\n",
    "Adjusting learning rate of group 0 to 4.5225e-04.\n",
    "In epoch 0, average training loss is 1.5519504678218872, average training acc is 0.4222226662708975.\n",
    "<ipython-input-131-bc8b6a24213a>:36: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
    "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
    "  with tqdm(total=val_step,desc=f'Val Epoch {e+1}/{total_epoch}',postfix=dict,mininterval=0.3) as pbar2:\n",
    "Val Epoch 1/5: 100%\n",
    "79/79 [00:15<00:00, 5.32it/s, Val Loss=1.43, Val accs=0.473]\n",
    "<ipython-input-131-bc8b6a24213a>:37: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
    "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
    "  for img, label in tqdm(iter(val_dataloader)):\n",
    "100%\n",
    "79/79 [00:15<00:00, 5.37it/s]\n",
    "In epoch 0, average validation loss is 1.4317884897883935, average validation acc is 0.4729034810126582.\n",
    "saving best model with acc 0.4729034810126582 at 0 epoch!\n",
    "Train Epoch 2/5: 100%\n",
    "391/391 [03:53<00:00, 1.90it/s, Train Loss=1.32, Tran accs=0.519]\n",
    "Adjusting learning rate of group 0 to 3.2725e-04.\n",
    "In epoch 1, average training loss is 1.323023446990401, average training acc is 0.5187340153147803.\n",
    "Val Epoch 2/5: 100%\n",
    "79/79 [00:15<00:00, 5.33it/s, Val Loss=1.28, Val accs=0.539]\n",
    "100%\n",
    "79/79 [00:15<00:00, 5.37it/s]\n",
    "In epoch 1, average validation loss is 1.2828171147575862, average validation acc is 0.5389636075949367.\n",
    "saving best model with acc 0.5389636075949367 at 1 epoch!\n",
    "Train Epoch 3/5: 100%\n",
    "391/391 [03:53<00:00, 1.89it/s, Train Loss=1.17, Tran accs=0.58]\n",
    "Adjusting learning rate of group 0 to 1.7275e-04.\n",
    "In epoch 2, average training loss is 1.1666057544291173, average training acc is 0.5795636189258312.\n",
    "Val Epoch 3/5: 100%\n",
    "79/79 [00:15<00:00, 5.32it/s, Val Loss=1.19, Val accs=0.571]\n",
    "100%\n",
    "79/79 [00:15<00:00, 5.41it/s]\n",
    "In epoch 2, average validation loss is 1.1929326887372174, average validation acc is 0.5714003164556962.\n",
    "saving best model with acc 0.5714003164556962 at 2 epoch!\n",
    "Train Epoch 4/5: 100%\n",
    "391/391 [03:53<00:00, 1.90it/s, Train Loss=1.02, Tran accs=0.633]\n",
    "Adjusting learning rate of group 0 to 4.7746e-05.\n",
    "In epoch 3, average training loss is 1.0190818402773278, average training acc is 0.6334958440812347.\n",
    "Val Epoch 4/5: 100%\n",
    "79/79 [00:15<00:00, 5.35it/s, Val Loss=1.05, Val accs=0.625]\n",
    "100%\n",
    "79/79 [00:14<00:00, 5.37it/s]\n",
    "In epoch 3, average validation loss is 1.0498902541172654, average validation acc is 0.6251977848101266.\n",
    "saving best model with acc 0.6251977848101266 at 3 epoch!\n",
    "Train Epoch 5/5: 100%\n",
    "391/391 [03:54<00:00, 1.89it/s, Train Loss=0.887, Tran accs=0.684]\n",
    "Adjusting learning rate of group 0 to 0.0000e+00.\n",
    "In epoch 4, average training loss is 0.8873024761219463, average training acc is 0.6840752876932968.\n",
    "Val Epoch 5/5: 100%\n",
    "79/79 [00:15<00:00, 5.33it/s, Val Loss=0.975, Val accs=0.653]\n",
    "100%\n",
    "79/79 [00:15<00:00, 5.40it/s]\n",
    "In epoch 4, average validation loss is 0.9753327437593967, average validation acc is 0.6529865506329114.\n",
    "saving best model with acc 0.6529865506329114 at 4 epoch!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}